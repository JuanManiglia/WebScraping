{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como científico de datos, tarde o temprano llegarás a un punto en el que tendrás que recopilar grandes cantidades de datos. Ya sea un proyecto o por pasatiempo y no siempre podremos contar con las API, pero tranquilo tenemos el web scraping... ¡Y una de las mejores herramientas de web scraping es Beautiful Soup!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Pero.... qué es el web scraping?\n",
    "\n",
    "En pocas palabras, el web scraping es la recopilación automatizada de datos de sitios web (para ser más precisos, del contenido HTML de los sitios web).\n",
    "\n",
    "En este Jupyter, aprenderás los conceptos básicos sobre cómo extraer datos de HTML. \n",
    "\n",
    "Lo harás extrayendo datos de la página de libros más vendidos de Book Depository, y para lograr esto, también tendrá que hacer uso de un poco de pandas principalmente.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conoce a tus nuevos mejores amigos: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Beautiful Soup\n",
    "- Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener la experiencia completa de Beautiful Soup, también deberás instalar un parser, dentro de ellos tenemos..\n",
    "\n",
    "- html.parser\n",
    "- lxml\n",
    "- html5lib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar el lxml ya que es el mas rápido "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se necesita una cosa más para que podamos comenzar a hacer web scraping, y es la biblioteca de ```requests```. Con ```requests``` podemos solicitar páginas web de sitios web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora asi manos a la obra.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mi primer scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre lo primero es importar las librerías "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, estamos listos para solicitar nuestra primera página web. No es nada complicado: guardamos la URL que queremos raspar en la variable URL, luego solicitamos la URL (requests.get (url)) y guardamos la respuesta en la variable de respuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.bookdepository.com/bestsellers\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo saber si se guardo correctamente el sitio web?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posibles respuestas:\n",
    "\n",
    "- [Respuestas informativas](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#information_responses) (100–199)\n",
    "- [Respuestas exitosas](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#successful_responses) (200–299)\n",
    "- [Mensajes de redirección](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#redirection_messages) (300–399)\n",
    "- [Respuestas de error del cliente](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#client_error_responses) (400–499)\n",
    "- [Respuestas de error del servidor](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#server_error_responses) (500–599)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero necesitamos el contenido HTML de la página web solicitada, así que como siguiente paso guardamos el contenido de la respuesta a html:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo podemos imprimir para ver su estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el resultado obtenido en HTML de la página de los libros más vendidos, pero es realmente difícil de leer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero para eso usamos BeautifulSoup y lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo lo hacemos?.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un objeto BeautifulSoup llamado soup con la siguiente línea de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bs?\n",
    "\n",
    "> from bs4 import BeautifulSoup as bs\n",
    "\n",
    "El primer parámetro del método bs() es html (que fue la variable en la que guardamos ese contenido HTML difícil de leer de la URL de los libros más vendidos)\n",
    "\n",
    "El segundo parámetro ('lxml'), es el parser que se usa en html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a ver el cambio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo navegar por un objeto de Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML consta de elementos como enlaces, párrafos, encabezados, bloques, etc. Estos elementos están envueltos entre etiquetas; dentro de la etiqueta de apertura y cierre se puede encontrar el contenido del elemento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](img\\html-content-web-scraping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los elementos HTML también pueden tener atributos que contienen información adicional sobre el elemento. Los atributos se definen en las etiquetas de apertura con la siguiente sintaxis: nombre del atributo = \"valor del atributo\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](img\\attribute-example-for-web-scraping-1536x386.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos aprendido algo de HTML básico, finalmente podemos comenzar a extraer datos de soup. Simplemente escriba un nombre de etiqueta después de soup y un punto (como soup.title), y observe cómo se desarrolla la magia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y sí queremos solo el texto?.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué sucede si solo necesita el atributo de un elemento? Tampoco hay problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos..\n",
    "> soup.a.get(\"href\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sintaxis de soup.```cualquier_etiqueta``` devuelve solo el primer elemento con ese nombre de etiqueta. En lugar de soup.```cualquier_etiqueta```, también puedes usar el método .find() y obtendrás exactamente el mismo resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sin utilizar .find()\")\n",
    "print(soup.h1)\n",
    "print(\"Utilizando .find()\")\n",
    "print(soup.find(\"h1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A menudo, no solo necesitas uno, sino todos los elementos (por ejemplo, cada enlace en una página). Para eso es bueno el método .find_all():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos fijamos podemos ver que lo que nos devuelve es una lista.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qué podemos hacer con una lista?.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_a = soup.find_all('a')\n",
    "for a in all_a[:5]:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok.. Pero como extraigo la data con BeautifilSoup?.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La página contiene 30 libros con información relacionada con ellos. De los datos disponibles extraeremos los siguientes:\n",
    "\n",
    "- book titles\n",
    "- formats (paperback or hardback)\n",
    "- publication dates\n",
    "- prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mientras trabajamos con BeautifulSoup, el flujo general de extracción de datos será un enfoque de dos pasos:\n",
    "\n",
    "* Inspeccionar en el navegador los elementos HTML que queremos extraer \n",
    "* Luego encontrar los elementos HTML con BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suficiente información..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manos a la obra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener los titulos de los libros (find_all + get_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello vamos a inspeccionar en el navegador (click derecho sobre un titulo de un libro y elegimos inspeccionar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_h3 = soup.find_all(\"h3\", class_=\"\")\n",
    "for h3 in all_h3:\n",
    "       print(h3.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips importantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soup.find_all(“h3”) encuentra cada elemento h3 en la página web; con class_=”title” especificamos que buscamos específicamente etiquetas h3 que contengan el atributo class_=”title” (nota importante: el “_” en class**__**=”title” no es un error tipográfico, se requiere en Beautiful Soup cuando seleccionando atributos de clase)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos los elementos h3 en all_h3, que se comporta como una lista, por lo que podemos recorrerlos con un bucle for. En cada iteración extraemos solo el texto del elemento h3 con .get_text(), y con el parámetro strip=True nos aseguramos de eliminar cualquier espacio en blanco innecesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener los formatos de los libros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del paso anterior tenemos todos los títulos de libros de la página de los más vendidos. Pero, ¿qué sabemos acerca de sus formatos? ¿Hay más libros de tapa dura o tapa blanda?\n",
    "\n",
    "Averigüémoslo inspeccionando el elemento de formato de libro:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre inspeccionamos y buscamos el formato.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y como queremos saber la cantidad de cada formato lo metemos en dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "div.item-info p.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats = soup.select(\"\") # div y p son etiquetas donde se encuentran\n",
    "formats_series = pd.Series(formats)\n",
    "formats_series.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener las fechas de publicación (find_all + get_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que antes inspeccionamos...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = soup.find_all(\"p\", class_=\"\")\n",
    "dates = [date.get_text()[-4:] for date in dates] #con esta list comprehension obtenemos solo el año\n",
    "dates_series = pd.Series(dates)\n",
    "dates_series.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener los precios (find_all + get_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspeccionamos.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = soup.find_all(\"p\", class_=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices[5].get_text(strip=True).split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_prices = []\n",
    "# for price in prices:\n",
    "#       original_price = price.find(\"span\", class_=\"rrp\")\n",
    "#       if original_price:\n",
    "#              current_price = str(original_price.previousSibling).strip() # nos quedamos solo con el numero sin etiquetas\n",
    "#              current_price = float(current_price.split(\"€\")[0].replace(\",\", \".\")) # quitamos el signo de euro y reemplazamos la coma por el punto para que python lo reconozca con float\n",
    "#              final_prices.append(current_price)\n",
    "#       else:\n",
    "#              current_price = float(price.get_text(strip=True).split(\"€\")[0].replace(\",\", \".\"))\n",
    "#              final_prices.append(current_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prices = []\n",
    "for price in prices:\n",
    "    original_price = price.find(\"span\", class_=\"sale-price\")\n",
    "    original_price = original_price.get_text()\n",
    "    original_price = float(original_price.split(\"€\")[0].replace(\",\", \".\"))\n",
    "    final_prices.append(original_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o en una solo linea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_price_compress = [float(price.find(\"span\", class_=\"sale-price\").get_text().split(\" €\")[0].replace(\",\", \".\")) for price in prices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recolectar información de un libro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero creamos un soup en la pagína 'principal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libros = soup.find_all(class_='item-img')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos en una variable la url principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_principal = 'https://www.bookdepository.com/es'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una lista con los urls de los libros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_URLs = []\n",
    "for libro in libros:\n",
    "    URL_libro = libro.find('a')['href']\n",
    "    lista_URLs.append(url_principal+URL_libro)\n",
    "\n",
    "lista_URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a analizar el url de un libro primero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un nuevo request para el primer libro: \n",
    "r = requests.get(lista_URLs[11])\n",
    "\n",
    "# Creamos una sopa específica con la info de cada libro\n",
    "soup_libro = bs(r.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creamos un soup del primer libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_libro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el titulo del libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = soup_libro.find('h1').text\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = soup_libro.find(class_ = 'rating-wrap hidden-md')('span')[5].text.split(' ')[-1].replace(',','.')\n",
    "rating = float(rating)\n",
    "rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cantidad de votaciones para el rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_count = soup_libro.find(class_ = 'rating-wrap hidden-md')('span')[-1].text.split(' ')[-6].replace('(','').replace('.','')\n",
    "rating_count = int(rating_count)\n",
    "rating_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tipo de formato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats = soup_libro.find(class_ = 'meta-info hidden-md')('li')[0].text\n",
    "formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author1 = soup_libro.find(class_ = 'item-info')('span')[-1].text\n",
    "author1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = float(soup_libro.find(class_ = 'sale-price').text.split(' ')[0].replace(',','.'))\n",
    "price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Url de la portada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagen = soup_libro.find(class_ = 'book-img')['src']\n",
    "imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora lo automatizamos para hacer un web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lista_libros)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('vips')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3014cd0240ca1f6ce0563b78067a7aa42945c9e3ac90c6ce8820ee7fdf547cd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
